{"cells":[{"cell_type":"markdown","metadata":{"id":"m9JJ7FBw84tG"},"source":["# Fase 1: Importar las dependencias"]},{"cell_type":"markdown","metadata":{"id":"t5DbIHC-F6Hf"},"source":["**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf\n","\n","**Ficheros** https://frogames.es/procesamiento-del-lenguaje-natural-moderno-en-python/\n","\n","**Ficheros** https://www.statmt.org/europarl/"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":416,"status":"ok","timestamp":1643223642127,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"ZbcvtPlp3YWu"},"outputs":[],"source":["import numpy as np\n","import math\n","import re #limpieza\n","import time\n","from google.colab import drive"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3328,"status":"ok","timestamp":1643223645454,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"P6o_cpZz3y_-"},"outputs":[],"source":["#solo se puede usar la última versión en Colab\n","\n","try:\n","    %tensorflow_version 2.x\n","except:\n","    pass\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","import tensorflow_datasets as tfds"]},{"cell_type":"markdown","metadata":{"id":"BQN8jwx48_yU"},"source":["# Fase 2: Pre Procesado de Datos\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bPlOT-2mlw0r"},"source":["## Carga de Ficheros"]},{"cell_type":"markdown","metadata":{"id":"dCD9jwXsLwS_"},"source":["Importamos los ficheros de nuestro Google Drive personal"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26608,"status":"ok","timestamp":1643223672060,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"eQpbl1pXCR0p","outputId":"395749ed-c51d-4fcd-d000-7d02f925b928"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"q8Or0sLV5b8t","executionInfo":{"status":"ok","timestamp":1643223691471,"user_tz":180,"elapsed":19414,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["with open(\"/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/Data/europarl-v7.es-en.en\", \n","          mode = \"r\", encoding = \"utf-8\") as f:\n","    europarl_en = f.read()\n","with open(\"/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/Data/europarl-v7.es-en.es\", \n","          mode = \"r\", encoding = \"utf-8\") as f:\n","    europarl_es = f.read()\n","with open(\"/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/Data/P85-Non-Breaking-Prefix.en\", \n","          mode = \"r\", encoding = \"utf-8\") as f:\n","    non_breaking_prefix_en = f.read()\n","with open(\"/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/Data/nonbreaking_prefix.es\", \n","          mode = \"r\", encoding = \"utf-8\") as f:\n","    non_breaking_prefix_es = f.read()\n","\n","#r: solo lectura"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1643223691472,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"TMAFFdpIyNZd","outputId":"2c7bf66d-f747-492d-bdf8-1f73cf2cb6a0"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"]},"metadata":{},"execution_count":5}],"source":["europarl_en[:100]"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1643223691472,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"BYgCMq6myYIi","outputId":"bdd818a4-d1aa-43d7-f191-a3f8611e5cb5"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo,'"]},"metadata":{},"execution_count":6}],"source":["europarl_es[:100]"]},{"cell_type":"markdown","metadata":{"id":"TEFw0D2vP_Dl"},"source":["## Limpieza de datos"]},{"cell_type":"markdown","metadata":{"id":"PwIBeGXn7LIJ"},"source":["Vamos a obtener los non_breaking_prefixes como una lista de palabras limpias con un punto al final para que nos sea más fácil de utilizar."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"L_TeuktU40Cb","executionInfo":{"status":"ok","timestamp":1643223691472,"user_tz":180,"elapsed":4,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["#pasaremos de un estring a una lista de string\n","#y sumamos un espacio y un punto\n","\n","non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n","non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n","non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n","non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]"]},{"cell_type":"markdown","metadata":{"id":"H9x4mZfKMaxD"},"source":["Necesitaremos cada palabra y otro símbolo que queramos mantener en minúsculas y separados por espacios para que podamos \"tokenizarlos\"."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Qg-8LLK-WdFp","executionInfo":{"status":"ok","timestamp":1643223751213,"user_tz":180,"elapsed":58283,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["corpus_en = europarl_en\n","\n","# Añadimos $$$ después de los puntos de frases sin fin de manera que se identifiquen facilmente y podamos eliminarlos después\n","for prefix in non_breaking_prefix_en:\n","    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n","corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en) #sub --> sustituir una expresión regulares\n","\n","# Eliminamos los marcadores $$$ que me sirven de localizante para eliminar los puntos que no eran puntos finales o punto seguido.\n","corpus_en = re.sub(r\"\\.\\$\\$\\$\", '', corpus_en)\n","\n","# Eliminamos espacios múltiples\n","corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n","corpus_en = corpus_en.split('\\n')\n","\n","\n","corpus_es = europarl_es\n","for prefix in non_breaking_prefix_es:\n","    corpus_es = corpus_es.replace(prefix, prefix + '$$$')\n","corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n","corpus_es = re.sub(r\"\\.\\$\\$\\$\", '', corpus_es)\n","corpus_es = re.sub(r\"  +\", \" \", corpus_es)\n","corpus_es = corpus_es.split('\\n')"]},{"cell_type":"markdown","metadata":{"id":"s-Y9v8-Tozl2"},"source":["## Tokenizar el Texto"]},{"cell_type":"markdown","metadata":{"id":"XCtIdCpmrpEa"},"source":["Tokenizar es convertir las palabras en números"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"p5YXanmOd_xK","executionInfo":{"status":"ok","timestamp":1643224704987,"user_tz":180,"elapsed":953776,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)\n","\n","tokenizer_es = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_es, target_vocab_size=2**13)\n","\n","#2 a la 13 es 8000 aprox --> la idea es achicar el vocabulario para optimizar el tiempo\n","#cdo es así el tokenizador identifica palabras que son similares"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ftIbPzIwCtwL","executionInfo":{"status":"ok","timestamp":1643224704988,"user_tz":180,"elapsed":4,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8198\n","VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2 # = 8225\n","#le sumo dos tokens para indicar principio y fin de frase"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"oPFe2YJDC9jw","executionInfo":{"status":"ok","timestamp":1643225021958,"user_tz":180,"elapsed":316973,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["#hemos convertido el corpus a numeros, y a cada frase le agrego el token de inicio y fin\n","\n","inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n","          for sentence in corpus_en]\n","outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1]\n","           for sentence in corpus_es]"]},{"cell_type":"markdown","metadata":{"id":"bG6AlcFMpC5C"},"source":["## Eliminamos las frases demasiado largas"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"F6CD6PLGyQWy","executionInfo":{"status":"ok","timestamp":1643225316355,"user_tz":180,"elapsed":294400,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["#Proceso de Padding, para sacar frases largas y completar las que son cortas.\n","\n","MAX_LENGTH = 20\n","idx_to_remove = [count for count, sent in enumerate(inputs)\n","                 if len(sent) > MAX_LENGTH]\n","#enumerate --> guarda el identificador de la fila con frase larga\n","\n","for idx in reversed(idx_to_remove): #LO HACE AL REVES para no cambiar el índice y borrar lo que no quería borra, lo que se corrio\n","    del inputs[idx]\n","    del outputs[idx]\n","\n","idx_to_remove = [count for count, sent in enumerate(outputs)\n","                 if len(sent) > MAX_LENGTH]\n","for idx in reversed(idx_to_remove):\n","    del inputs[idx]\n","    del outputs[idx]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ZmZ06J2l1tIp","executionInfo":{"status":"ok","timestamp":1643225316356,"user_tz":180,"elapsed":3,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["#import pandas as pd \n","\n","#pd.DataFrame(inputs).to_csv('/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/Data/inputs.csv', index=False)\n","#pd.DataFrame(outputs).to_csv('/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/Data/outputs.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"Ypm8h5aZQTZ1"},"source":["## Creamos las entradas y las salidas"]},{"cell_type":"markdown","metadata":{"id":"9FP0WPsdM8hl"},"source":["A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"nvDfLDWUONlE","executionInfo":{"status":"ok","timestamp":1643225320772,"user_tz":180,"elapsed":4419,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["\n","inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n","                                                       value=0, #cdo la frase no llega 20 tokens, se completa con 0\n","                                                       padding='post',\n","                                                       maxlen=MAX_LENGTH)\n","outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n","                                                        value=0,\n","                                                        padding='post',\n","                                                        maxlen=MAX_LENGTH)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"wFxMp3TOIYff","executionInfo":{"status":"ok","timestamp":1643225321339,"user_tz":180,"elapsed":575,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n","\n","dataset = dataset.cache() #incrementa la velocidad para acceso de datos\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE) #para mezclar los datos y se preparan de grupos de 64 frases para el entrenamiento\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) #para mejorar el acceso a los datos"]},{"cell_type":"markdown","metadata":{"id":"ycT0YqydRcUd"},"source":["# Fase 3: Construcción del Modelo"]},{"cell_type":"markdown","metadata":{"id":"-SBoH8G4XyR9"},"source":["## Embedding"]},{"cell_type":"markdown","metadata":{"id":"7G9C3ucmJ86I"},"source":["Fórmula de la Codificación Posicional:\n","\n","$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n","\n","$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"e2wc6sYlX0dr","executionInfo":{"status":"ok","timestamp":1643225321340,"user_tz":180,"elapsed":4,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class PositionalEncoding(layers.Layer):\n","\n","    def __init__(self):\n","        super(PositionalEncoding, self).__init__()\n","    \n","    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n","        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n","        return pos * angles # (seq_length, d_model)\n","\n","    def call(self, inputs):\n","        seq_length = inputs.shape.as_list()[-2]\n","        d_model = inputs.shape.as_list()[-1]\n","        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n","                                 np.arange(d_model)[np.newaxis, :],\n","                                 d_model)\n","        angles[:, 0::2] = np.sin(angles[:, 0::2]) #pares\n","        angles[:, 1::2] = np.cos(angles[:, 1::2]) #impares\n","        pos_encoding = angles[np.newaxis, ...]\n","        return inputs + tf.cast(pos_encoding, tf.float32)\n","\n","        #aquí termina el positional encoding, estamos a la primera entrada del modelo"]},{"cell_type":"markdown","metadata":{"id":"lcw8YIQqRhOJ"},"source":["## Attention"]},{"cell_type":"markdown","metadata":{"id":"3sffhwwvX-wj"},"source":["### Cálculo de la Atención"]},{"cell_type":"markdown","metadata":{"id":"7VBuW6lESLDX"},"source":["$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2rEoCNJURbrT","executionInfo":{"status":"ok","timestamp":1643225321340,"user_tz":180,"elapsed":4,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["def scaled_dot_product_attention(queries, keys, values, mask):\n","    product = tf.matmul(queries, keys, transpose_b=True)\n","    \n","    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n","    scaled_product = product / tf.math.sqrt(keys_dim)\n","    \n","    if mask is not None:\n","        scaled_product += (mask * -1e9)\n","    \n","    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n","    \n","    return attention"]},{"cell_type":"markdown","metadata":{"id":"-MjtvXrfYEx7"},"source":["### Sub capa de atención de encabezado múltiple"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"lvq4I9uTX5p7","executionInfo":{"status":"ok","timestamp":1643225321340,"user_tz":180,"elapsed":3,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class MultiHeadAttention(layers.Layer):\n","    \n","    def __init__(self, nb_proj):\n","        super(MultiHeadAttention, self).__init__()\n","        self.nb_proj = nb_proj\n","        \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        assert self.d_model % self.nb_proj == 0\n","        \n","        self.d_proj = self.d_model // self.nb_proj\n","        \n","        self.query_lin = layers.Dense(units=self.d_model)\n","        self.key_lin = layers.Dense(units=self.d_model)\n","        self.value_lin = layers.Dense(units=self.d_model)\n","        \n","        self.final_lin = layers.Dense(units=self.d_model)\n","        \n","    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n","        shape = (batch_size,\n","                 -1,\n","                 self.nb_proj,\n","                 self.d_proj)\n","        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n","        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n","    \n","    def call(self, queries, keys, values, mask):\n","        batch_size = tf.shape(queries)[0]\n","        \n","        queries = self.query_lin(queries)\n","        keys = self.key_lin(keys)\n","        values = self.value_lin(values)\n","        \n","        queries = self.split_proj(queries, batch_size)\n","        keys = self.split_proj(keys, batch_size)\n","        values = self.split_proj(values, batch_size)\n","        \n","        attention = scaled_dot_product_attention(queries, keys, values, mask)\n","        \n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        \n","        concat_attention = tf.reshape(attention,\n","                                      shape=(batch_size, -1, self.d_model))\n","        \n","        outputs = self.final_lin(concat_attention)\n","        \n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"yiyuHe1OeT5N"},"source":["## Codificación"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"UV0ZMH7KT_KZ","executionInfo":{"status":"ok","timestamp":1643225321341,"user_tz":180,"elapsed":4,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class EncoderLayer(layers.Layer):\n","    \n","    def __init__(self, FFN_units, nb_proj, dropout_rate):\n","        super(EncoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.nb_proj = nb_proj\n","        self.dropout_rate = dropout_rate\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        \n","        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n","        self.dense_2 = layers.Dense(units=self.d_model)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","    def call(self, inputs, mask, training):\n","        attention = self.multi_head_attention(inputs,\n","                                              inputs,\n","                                              inputs,\n","                                              mask)\n","        attention = self.dropout_1(attention, training=training)\n","        attention = self.norm_1(attention + inputs)\n","        \n","        outputs = self.dense_1(attention)\n","        outputs = self.dense_2(outputs)\n","        outputs = self.dropout_2(outputs, training=training)\n","        outputs = self.norm_2(outputs + attention)\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"P-P92KeZih60","executionInfo":{"status":"ok","timestamp":1643225321341,"user_tz":180,"elapsed":4,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class Encoder(layers.Layer):\n","    \n","    def __init__(self,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout_rate,\n","                 vocab_size,\n","                 d_model,\n","                 name=\"encoder\"):\n","        super(Encoder, self).__init__(name=name)\n","        self.nb_layers = nb_layers\n","        self.d_model = d_model\n","        \n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        self.enc_layers = [EncoderLayer(FFN_units,\n","                                        nb_proj,\n","                                        dropout_rate) \n","                           for _ in range(nb_layers)]\n","    \n","    def call(self, inputs, mask, training):\n","        outputs = self.embedding(inputs)\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","        \n","        for i in range(self.nb_layers):\n","            outputs = self.enc_layers[i](outputs, mask, training)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"7DthraBEwuvl"},"source":["## Descodificación"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"7ZWZyFBnwy8u","executionInfo":{"status":"ok","timestamp":1643225321929,"user_tz":180,"elapsed":592,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class DecoderLayer(layers.Layer):\n","    \n","    def __init__(self, FFN_units, nb_proj, dropout_rate):\n","        super(DecoderLayer, self).__init__()\n","        self.FFN_units = FFN_units\n","        self.nb_proj = nb_proj\n","        self.dropout_rate = dropout_rate\n","    \n","    def build(self, input_shape):\n","        self.d_model = input_shape[-1]\n","        \n","        # Self multi head attention\n","        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","        # Multi head attention combinado con la salida del encoder \n","        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","        # Feed foward\n","        self.dense_1 = layers.Dense(units=self.FFN_units,\n","                                    activation=\"relu\")\n","        self.dense_2 = layers.Dense(units=self.d_model)\n","        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n","        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n","        \n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        attention = self.multi_head_attention_1(inputs,\n","                                                inputs,\n","                                                inputs,\n","                                                mask_1)\n","        attention = self.dropout_1(attention, training)\n","        attention = self.norm_1(attention + inputs)\n","        \n","        attention_2 = self.multi_head_attention_2(attention,\n","                                                  enc_outputs,\n","                                                  enc_outputs,\n","                                                  mask_2)\n","        attention_2 = self.dropout_2(attention_2, training)\n","        attention_2 = self.norm_2(attention_2 + attention)\n","        \n","        outputs = self.dense_1(attention_2)\n","        outputs = self.dense_2(outputs)\n","        outputs = self.dropout_3(outputs, training)\n","        outputs = self.norm_3(outputs + attention_2)\n","        \n","        return outputs"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"kpzdiWHiwywF","executionInfo":{"status":"ok","timestamp":1643225321930,"user_tz":180,"elapsed":3,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class Decoder(layers.Layer):\n","    \n","    def __init__(self,\n","                 nb_layers, #Nx ctas veces pasara\n","                 FFN_units, #número de neuronas\n","                 nb_proj, #numero de proyecciones\n","                 dropout_rate, #cta neuronas apagamos en el training\n","                 vocab_size, #tamaño del voacbulario que viene del tokenizador\n","                 d_model,\n","                 name=\"decoder\"):\n","        super(Decoder, self).__init__(name=name)\n","        self.d_model = d_model\n","        self.nb_layers = nb_layers\n","        \n","        self.embedding = layers.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding()\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        \n","        self.dec_layers = [DecoderLayer(FFN_units,\n","                                        nb_proj,\n","                                        dropout_rate) \n","                           for _ in range(nb_layers)]\n","    \n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n","        outputs = self.embedding(inputs)\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        outputs = self.pos_encoding(outputs)\n","        outputs = self.dropout(outputs, training)\n","        \n","        for i in range(self.nb_layers):\n","            outputs = self.dec_layers[i](outputs,\n","                                         enc_outputs,\n","                                         mask_1,\n","                                         mask_2,\n","                                         training)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"x5sJYkjbz5DD"},"source":["## Transformer"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1643225321930,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"GqvqNjJPwyh-"},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","    \n","    def __init__(self,\n","                 vocab_size_enc,\n","                 vocab_size_dec,\n","                 d_model,\n","                 nb_layers,\n","                 FFN_units,\n","                 nb_proj,\n","                 dropout_rate,\n","                 name=\"transformer\"):\n","        super(Transformer, self).__init__(name=name)\n","        \n","        self.encoder = Encoder(nb_layers,\n","                               FFN_units,\n","                               nb_proj,\n","                               dropout_rate,\n","                               vocab_size_enc,\n","                               d_model)\n","        self.decoder = Decoder(nb_layers,\n","                               FFN_units,\n","                               nb_proj,\n","                               dropout_rate,\n","                               vocab_size_dec,\n","                               d_model)\n","        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n","    \n","    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n","        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","        return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","    def create_look_ahead_mask(self, seq):\n","        seq_len = tf.shape(seq)[1]\n","        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","        return look_ahead_mask\n","    \n","    def call(self, enc_inputs, dec_inputs, training):\n","        enc_mask = self.create_padding_mask(enc_inputs)\n","        dec_mask_1 = tf.maximum(\n","            self.create_padding_mask(dec_inputs),\n","            self.create_look_ahead_mask(dec_inputs)\n","        )\n","        dec_mask_2 = self.create_padding_mask(enc_inputs)\n","        \n","        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n","        dec_outputs = self.decoder(dec_inputs,\n","                                   enc_outputs,\n","                                   dec_mask_1,\n","                                   dec_mask_2,\n","                                   training)\n","        \n","        outputs = self.last_linear(dec_outputs)\n","        \n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"-c-LRThUPrso"},"source":["# Entrenamiento"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"qiOdqQ5qPs8z","executionInfo":{"status":"ok","timestamp":1643225321930,"user_tz":180,"elapsed":3,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["tf.keras.backend.clear_session()\n","\n","# Hiper Parámetros (achicados para optimizar tiempos)\n","D_MODEL = 128 # 512\n","NB_LAYERS = 4 # 6\n","FFN_UNITS = 512 # 2048\n","NB_PROJ = 8 # 8\n","DROPOUT_RATE = 0.1 # 0.1\n","\n","transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n","                          vocab_size_dec=VOCAB_SIZE_ES,\n","                          d_model=D_MODEL,\n","                          nb_layers=NB_LAYERS,\n","                          FFN_units=FFN_UNITS,\n","                          nb_proj=NB_PROJ,\n","                          dropout_rate=DROPOUT_RATE)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"46xg4Wrg1Wgl","executionInfo":{"status":"ok","timestamp":1643225321930,"user_tz":180,"elapsed":2,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n","                                                            reduction=\"none\")\n","\n","def loss_function(target, pred):\n","    mask = tf.math.logical_not(tf.math.equal(target, 0))\n","    loss_ = loss_object(target, pred)\n","    \n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_mean(loss_)\n","\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"4Goque362343","executionInfo":{"status":"ok","timestamp":1643225321930,"user_tz":180,"elapsed":2,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","        \n","        self.d_model = tf.cast(d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","        \n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","leaning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(leaning_rate,\n","                                     beta_1=0.9,\n","                                     beta_2=0.98,\n","                                     epsilon=1e-9)\n","        "]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2520,"status":"ok","timestamp":1643225691279,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"},"user_tz":180},"id":"Nb_32PIU5Zkh","outputId":"53c2087d-acdb-42b1-a4ab-4d7c0090ca85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Último checkpoint restaurado!!\n"]}],"source":["checkpoint_path = \"/content/drive/Othercomputers/Mi PC/02. Programacion/Proyectos/Traductor con tecnología Transformer/ckpt\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print(\"Último checkpoint restaurado!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhFK5kUx602K"},"outputs":[],"source":["EPOCHS = 10\n","for epoch in range(EPOCHS):\n","    print(\"Inicio del epoch {}\".format(epoch+1))\n","    start = time.time()\n","    \n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","    \n","    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n","        dec_inputs = targets[:, :-1]\n","        dec_outputs_real = targets[:, 1:]\n","        with tf.GradientTape() as tape:\n","            predictions = transformer(enc_inputs, dec_inputs, True)\n","            loss = loss_function(dec_outputs_real, predictions)\n","        \n","        gradients = tape.gradient(loss, transformer.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","        \n","        train_loss(loss)\n","        train_accuracy(dec_outputs_real, predictions)\n","        \n","        if batch % 50 == 0:\n","            print(\"Epoch {} Lote {} Pérdida {:.4f} Precisión {:.4f}\".format(\n","                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n","            \n","    ckpt_save_path = ckpt_manager.save()\n","    print(\"Guardando checkpoint para el epoch {} en {}\".format(epoch+1,\n","                                                        ckpt_save_path))\n","    print(\"Tiempo que ha tardado 1 epoch: {} segs\\n\".format(time.time() - start))"]},{"cell_type":"markdown","metadata":{"id":"nmzyRwDrRGdq"},"source":["# Evaluación"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"cNHwJJrz3lPB","executionInfo":{"status":"ok","timestamp":1643234500661,"user_tz":180,"elapsed":433,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["def evaluate(inp_sentence):\n","    inp_sentence = \\\n","        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n","    enc_input = tf.expand_dims(inp_sentence, axis=0)\n","    \n","    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n","    \n","    for _ in range(MAX_LENGTH):\n","        predictions = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n","        \n","        prediction = predictions[:, -1:, :]\n","        \n","        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n","        \n","        if predicted_id == VOCAB_SIZE_ES-1:\n","            return tf.squeeze(output, axis=0)\n","        \n","        output = tf.concat([output, predicted_id], axis=-1)\n","        \n","    return tf.squeeze(output, axis=0)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"s6VeFKrE6Kdx","executionInfo":{"status":"ok","timestamp":1643234504053,"user_tz":180,"elapsed":539,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}}},"outputs":[],"source":["def translate(sentence):\n","    output = evaluate(sentence).numpy()\n","    \n","    predicted_sentence = tokenizer_es.decode(\n","        [i for i in output if i < VOCAB_SIZE_ES-2]\n","    )\n","    \n","    print(\"Entrada: {}\".format(sentence))\n","    print(\"Traducción predicha: {}\".format(predicted_sentence))"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"BupFjJlgDvCA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643235517711,"user_tz":180,"elapsed":1415,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}},"outputId":"3cbff524-94b5-4df2-efa8-361433f423d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada: This is my first translator.\n","Traducción predicha: Este es mi primera transición.\n"]}],"source":["translate(\"This is my first translator.\")"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"ZdoWKbCP7Czs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643234859688,"user_tz":180,"elapsed":1929,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}},"outputId":"bfb34ded-2cf0-4bd0-dc3f-f872aa3ce09f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada: This is a really powerful tool!\n","Traducción predicha: ¡Es un instrumento realmente poderoso y poderosa!\n"]}],"source":["translate(\"This is a really powerful tool!\")"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"UGjBEb5WFMGt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643234873005,"user_tz":180,"elapsed":2623,"user":{"displayName":"William Wallace","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02462327362597608294"}},"outputId":"4a83a7cd-03d8-4218-e682-3b73243d5540"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada: This is an interesting course about Natural Language Processing\n","Traducción predicha: Este es un informe interesante sobre el programa de Letonia de Navidad\n"]}],"source":["translate(\"This is an interesting course about Natural Language Processing\")"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Traductor_Transformer_para_NLP.ipynb","provenance":[{"file_id":"16JmM4d_Wc3Eh1-iyjkA5_kw8qLDDFhr2","timestamp":1643135574655},{"file_id":"1i4a26jVRVsAIfAGCe5pnl4l51B6GvjWs","timestamp":1595454386969},{"file_id":"17dOcF-VlAVBY0vzgqaANGQ4taSNc2Wp6","timestamp":1571411676663}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}